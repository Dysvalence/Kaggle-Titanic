{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training Examples:', 891)\n",
      "('Test Examples:', 418)\n",
      "('All Examples:', 1309)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This script imports the Kaggle Titanic dataset in .csv form, extracts a new feature from name data, imputates missing\n",
    "values using an autoencoder initialized neural network set for regression. Lastly it builds an autoencoder initalized\n",
    "deep neural network to determine if an individual in the test set is likely to have survived. Keras is used for all NNs.\n",
    "\n",
    "Originally I randomly partitioned off 91 of 891 examples for validation testing, however, training accuracy was severely \n",
    "affected and test accuracy was very erratic depending on which 91 were chosen, i.e. insufficent data. As my primary goals \n",
    "were to practice using Pandas and neural networks, I did not look at other ways to deal with this. However, I ran the \n",
    "autoencoder on the entire dataset during training, including the test set. This obviously risks overfitting the test set,\n",
    "but I wanted to see if it would help, as that would suggest that mixing unlabeled and labeled data in such a fashion may \n",
    "be useful. \n",
    "\n",
    "There is also a massive bug- monitoring training via the verbosity setting on Keras results in Jupyter bugs. The latest\n",
    "update fixed the crashing, but now the latency is horrendous. To work around this, multiple models with increasing epochs\n",
    "are run to easily pick the best one, at the cost of a significant increase in complexity. When autoencoder initialized \n",
    "layers are used, I accidentally set it to shallow copy, and as such it is trained by every model that touches it. For some\n",
    "reason this helps by a few percent, and fixing it makes it worse, even with more epochs during the AE initialization. This\n",
    "may help counter the vanishing gradient problem in a manner reminiscent of deep belief networks, but further research on \n",
    "larger, and more varied data sets with more test runs are necessary.\n",
    "\n",
    "I feel the need to include the disclaimer that hammering everything with complex models and large deep nets\n",
    "may not always be optimal, and that this script is a little haphazard in that regard. Compared to some other public SVM \n",
    "and random forest based models on the leaderboard, this is exceptionally slow even with GPU acceleration. Time permitting,\n",
    "I want to explore combining these methods together in a committee/ensemble, as that has ranked well on the leaderboard.\n",
    "\"\"\"\n",
    "\n",
    "import csv\n",
    "import numpy\n",
    "import pandas\n",
    "\n",
    "dfTrain=pandas.read_csv('./Titanic/train.csv')\n",
    "dfTest=pandas.read_csv('./Titanic/test.csv')\n",
    "\n",
    "print('Training Examples:',len(dfTrain))\n",
    "print('Test Examples:',len(dfTest))\n",
    "dfAll=dfTrain.append(dfTest) \n",
    "print('All Examples:',len(dfAll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Determine social ranking from names to create an additional feature\n",
    "dfAll['Title']=dfAll['Name'].apply( lambda x:( (x.split(', ')[1]).split(' ')[0] ) )\n",
    "\n",
    "dfAll['Title']=dfAll['Title'].replace(to_replace=['Don.','Rev.','Master.','Dr.','Col.','Capt.',\n",
    "                                                      'Major.','Jonkheer.','Lady.','the','Sir.','Dona.'], value=1)                                          \n",
    "dfAll['Title']=dfAll['Title'].replace(to_replace=['Miss.','Mlle.','Ms.','Mrs.','Mme.','Mr.'],value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Simple imputation, based on this analysis: https://www.kaggle.com/mrisdal/titanic/exploring-survival-on-the-titanic\n",
    "dfAll['Fare']=dfAll['Fare'].replace(to_replace=[dfTest['Fare'][1044-892]],value=8.05)\n",
    "#Numpy does not allow direct comparison to np.nan. This is an admittedly ugly workaround.\n",
    "NAN=(dfTrain['Embarked'][829])\n",
    "dfAll['Embarked']=dfTrain['Embarked'].replace(to_replace=[NAN],value='C')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#one-hot encoding\n",
    "dfAll=pandas.get_dummies(dfAll, columns=['Sex'])\n",
    "dfAll=pandas.get_dummies(dfAll, columns=['Embarked'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Normalization\n",
    "def normalize(df,strlab):\n",
    "    df[strlab]=(df[strlab].map(lambda x: float(float(float(x) - float(numpy.min(df[strlab]))) /\n",
    "                                                     float(float(numpy.max(df[strlab])) - float(numpy.min(df[strlab]))))))\n",
    "normalize(dfAll,'Age')\n",
    "normalize(dfAll,'Parch')\n",
    "normalize(dfAll,'Pclass')\n",
    "normalize(dfAll,'Fare')\n",
    "normalize(dfAll,'SibSp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Has age:', 1046)\n",
      "('Missing age:', 263)\n"
     ]
    }
   ],
   "source": [
    "#Prepare to imputate age.\n",
    "dfNoAge=dfAll[numpy.isnan(dfAll['Age'])]\n",
    "dfAge=dfAll[(numpy.isnan(dfAll['Age']))==False]\n",
    "print('Has age:',len(dfAge))\n",
    "print('Missing age:',len(dfNoAge))\n",
    "\n",
    "noAgeAE=dfAll.drop(['Age','Cabin','PassengerId','Name','Survived','Ticket'],axis=1).as_matrix()\n",
    "ageTrainX=dfAge.drop(['Age','Cabin','PassengerId','Name','Survived','Ticket'],axis=1).as_matrix()\n",
    "ageTrainY=dfAge.as_matrix(columns=['Age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GTX 960 (CNMeM is enabled with initial size: 70.0% of memory, cuDNN 4007)\n"
     ]
    }
   ],
   "source": [
    "#import Keras \n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1309/1309 [==============================] - 0s     \n",
      "('Test score:', 0.61237944219748053)\n",
      "('Test accuracy:', 0.00076394194041252863)\n",
      "\n",
      "20\n",
      "1309/1309 [==============================] - 0s     \n",
      "('Test score:', 0.030432480962974046)\n",
      "('Test accuracy:', 0.5255920549582852)\n",
      "\n",
      "40\n",
      "1309/1309 [==============================] - 0s     \n",
      "('Test score:', 0.015906905431678907)\n",
      "('Test accuracy:', 0.16348357532227467)\n",
      "\n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "#Python does not have a Multiple Imputation by Chained Equations library.\n",
    "#While I could export to R and use MICE I wanted to try neural net regression.\n",
    "#As my research frequently involves autoencoders, I used an AE to initialize the hidden before running a normal NN\n",
    "\n",
    "batch_size = 20\n",
    "nb_classes = 2\n",
    "nb_epochs = 20\n",
    "models=[]\n",
    "\n",
    "#Runs multiple models with increasing epochs, and with the option to pick an earlier one.\n",
    "#Not optimal, but helps deal with bugs and latency in jupyter notebook. Verbosity is disabled for this reason.\n",
    "for x in range(0,3):\n",
    "    nb_epoch=nb_epochs*x\n",
    "    print(nb_epoch)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(9, input_shape=(10,)))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('linear'))\n",
    "\n",
    "    model.compile(loss='mean_absolute_error',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(noAgeAE, noAgeAE,\n",
    "                    batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "                    verbose=0, validation_data=(noAgeAE, noAgeAE))\n",
    "\n",
    "    score = model.evaluate(noAgeAE, noAgeAE, verbose=1)\n",
    "    models.append(model)\n",
    "    print('Test score:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "    print('')\n",
    "print('\\nDone')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "AELayer=model #Selects last model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1046/1046 [==============================] - 0s     \n",
      "('Test score:', 0.79905932161593751)\n",
      "('Test accuracy:', 0.00095602294455066918)\n",
      "\n",
      "20\n",
      "1046/1046 [==============================] - 0s     \n",
      "('Test score:', 0.023517788706585965)\n",
      "('Test accuracy:', 0.00095602294455066918)\n",
      "\n",
      "40\n",
      "1046/1046 [==============================] - 0s     \n",
      "('Test score:', 0.019936984493958108)\n",
      "('Test accuracy:', 0.0019120458891013384)\n",
      "\n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "nb_classes = 2\n",
    "nb_epochs = 20\n",
    "models=[]\n",
    "\n",
    "#Massive bug here in which each run is not independent, and the initialized layers get backproped over on every run.\n",
    "#However, this seems to increase performance, and may be worth looking into.\n",
    "\n",
    "for x in range(0,3):\n",
    "    nb_epoch=nb_epochs*x\n",
    "    print(nb_epoch)\n",
    "    model = Sequential()\n",
    "    model.add(AELayer.layers[0])\n",
    "    model.add(AELayer.layers[1])\n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('linear'))\n",
    "\n",
    "    model.compile(loss='mean_squared_error',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(ageTrainX, ageTrainY,\n",
    "                    batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "                    verbose=0, validation_data=(ageTrainX, ageTrainY))\n",
    "\n",
    "    score = model.evaluate(ageTrainX, ageTrainY, verbose=1)\n",
    "    models.append(model)\n",
    "    print('Test score:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "    print('')\n",
    "print('\\nDone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "finmod=model #pick last model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#imputate age from NN model\n",
    "\n",
    "dfNoAge=dfNoAge.drop(['Age'],axis=1)\n",
    "matNoAge=dfNoAge.drop(['Cabin','PassengerId','Name','Survived','Ticket'], axis=1)\n",
    "\n",
    "ages=finmod.predict(matNoAge.as_matrix(),batch_size=1)\n",
    "\n",
    "dfNoAge['Age']=ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Recombine data, and resplit to build the classifier\n",
    "dfRecombined=dfNoAge.append(dfAge)\n",
    "dfRecombined=dfRecombined.drop(['Cabin','Name','Ticket'],axis=1)\n",
    "MatAEClass=dfRecombined.drop(['Survived','PassengerId'],axis=1).as_matrix()\n",
    "\n",
    "dfRecTest=dfRecombined[numpy.isnan(dfRecombined['Survived'])]\n",
    "dfRecTest=dfRecTest.drop(['Survived'],axis=1)\n",
    "dfRecTrain=dfRecombined[numpy.isnan(dfRecombined['Survived'])==False]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1309/1309 [==============================] - 0s     \n",
      "('Test score:', 0.45282051752686409)\n",
      "('Test accuracy:', 0.0)\n",
      "\n",
      "20\n",
      "1309/1309 [==============================] - 0s     \n",
      "('Test score:', 0.0014404552123197009)\n",
      "('Test accuracy:', 0.33155080243501162)\n",
      "\n",
      "40\n",
      "1309/1309 [==============================] - 0s     \n",
      "('Test score:', 0.0012919703258181558)\n",
      "('Test accuracy:', 0.16883116897915593)\n",
      "\n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "I tried splitting off 91 examples for validation but training accuracy was hit pretty hard,\n",
    "and validation accuracy was extremely erratic depending on how the random sampling went.\n",
    "Since nothing is going right I decided to risk deliberately overfitting the test set\n",
    "and decided to initialize the first layer of a deep neural net by running an AE on all the data.\n",
    "This might be a viable way to use data with missing labels.\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 1\n",
    "nb_classes = 2\n",
    "nb_epochs = 20\n",
    "models=[]\n",
    "\n",
    "for x in range(0,3):\n",
    "    nb_epoch=nb_epochs*x\n",
    "    print(nb_epoch)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(7, input_shape=(11,)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(11))\n",
    "    model.add(Activation('linear'))\n",
    "\n",
    "    model.compile(loss='mean_squared_error',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#Verbosity disabled since it triggers a juptyer notebook bug and crashes the training\n",
    "\n",
    "    history = model.fit(MatAEClass, MatAEClass,\n",
    "                    batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "                    verbose=0, validation_data=(MatAEClass, MatAEClass))\n",
    "\n",
    "    score = model.evaluate(MatAEClass, MatAEClass, verbose=1)\n",
    "    models.append(model)\n",
    "    print('Test score:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "    print('')\n",
    "print('\\nDone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AEModel=model #Pick model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Reseperate training and test data; format labels\n",
    "finTrainX=dfRecTrain.drop(['Survived','PassengerId'],axis=1).as_matrix()\n",
    "dfRecTrain=pandas.get_dummies(dfRecTrain, columns=['Survived'])\n",
    "finTrainY=dfRecTrain.as_matrix(columns=['Survived_0.0','Survived_1.0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "891/891 [==============================] - 0s     \n",
      "('Test score:', 0.68580062568655986)\n",
      "('Test accuracy:', 0.61616161629540889)\n",
      "\n",
      "10\n",
      "891/891 [==============================] - 0s     \n",
      "('Test score:', 0.43271631062632859)\n",
      "('Test accuracy:', 0.81144781164850044)\n",
      "\n",
      "20\n",
      "891/891 [==============================] - 0s     \n",
      "('Test score:', 0.40816340388941308)\n",
      "('Test accuracy:', 0.83277216630618844)\n",
      "\n",
      "30\n",
      "891/891 [==============================] - 0s     \n",
      "('Test score:', 0.40780358653678639)\n",
      "('Test accuracy:', 0.82716049469681296)\n",
      "\n",
      "40\n",
      "891/891 [==============================] - 0s     \n",
      "('Test score:', 0.38683252218864045)\n",
      "('Test accuracy:', 0.83950617350846968)\n",
      "\n",
      "50\n",
      "891/891 [==============================] - 0s     \n",
      "('Test score:', 0.37396662708217182)\n",
      "('Test accuracy:', 0.84511784531853418)\n",
      "\n",
      "60\n",
      "891/891 [==============================] - 0s     \n",
      "('Test score:', 0.36696473029928173)\n",
      "('Test accuracy:', 0.84848484801657409)\n",
      "\n",
      "70\n",
      "891/891 [==============================] - 0s     \n",
      "('Test score:', 0.35732340454787653)\n",
      "('Test accuracy:', 0.85297418630751964)\n",
      "\n",
      "80\n",
      "891/891 [==============================] - 0s     \n",
      "('Test score:', 0.36090157477877088)\n",
      "('Test accuracy:', 0.84399551086286639)\n",
      "\n",
      "90\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Again, theres the massive bug where the first hidden layer is shared by, and subsequently trained by, all the models.\n",
    "However this has given me the best test accuracy so far, and fixing this bug drops accuracy by a few percent at least.\n",
    "Given that deep nets tend to have vanishing gradients further up this may be a way to counter that, similar to how AE based\n",
    "deep belief networks are used.\n",
    "\n",
    "I've messed around a lot with varying layers, hidden units, activations, optimizers, loss functions, dropout \n",
    "and regularization, and while the AE initialization helped, getting further with just NN/DNN architectures will\n",
    "require either luck or experience. Other public submissions further up the leaderboard have used genetic algorithms and\n",
    "multiple models in a committee. Integrating elements of this approach may be helpful.\n",
    "\"\"\"\n",
    "batch_size = 1\n",
    "nb_classes = 2\n",
    "nb_epochs = 10 \n",
    "models=[]\n",
    "\n",
    "for x in range(0,20):\n",
    "    nb_epoch=nb_epochs*x\n",
    "    print(nb_epoch)\n",
    "    model = Sequential()\n",
    "    model.add(AEModel.layers[0])\n",
    "    model.add(AEModel.layers[1])\n",
    "    #model.add(Dense(100, input_shape=(11,)))\n",
    "    #model.add(Activation('relu'))\n",
    "    #model.add(Dropout(.2))\n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('relu'))\n",
    "    #model.add(Dropout(.2))\n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#Verbosity disabled since it triggers a juptyer notebook bug and crashes the training\n",
    "\n",
    "    history = model.fit(finTrainX, finTrainY,\n",
    "                    batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "                    verbose=0, validation_data=(finTrainX, finTrainY))\n",
    "\n",
    "    score = model.evaluate(finTrainX, finTrainY, verbose=1)\n",
    "    models.append(model)\n",
    "    print('Test score:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "    print('')\n",
    "print('\\nDone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Converts things to CSV for Kaggle submission\n",
    "\n",
    "model=models[-1]\n",
    "\n",
    "outs = model.predict_classes(dfRecTest.drop(['PassengerId'],axis=1).as_matrix())\n",
    "print(outs)\n",
    "\n",
    "output = []\n",
    "output.append([\"PassengerId\",\"Survived\"])\n",
    "matTestId=dfRecTest.as_matrix(columns=['PassengerId'])\n",
    "for x in range(0,len(outs)):\n",
    "    output.append([matTestId[x][0],int(outs[x])])\n",
    "    \n",
    "with open(\"outputAENNRAEDNN.csv\", 'wb') as f:\n",
    "    writer=csv.writer(f)\n",
    "    writer.writerows(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
